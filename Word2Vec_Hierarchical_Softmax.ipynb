{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJSDKS-42Sx8",
        "outputId": "6dc8d45d-3c5d-4465-a9dd-4ce6b2dd8800"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 300 characters:\n",
            " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organiz\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Download the dataset\n",
        "url = 'http://mattmahoney.net/dc/text8.zip'\n",
        "filename = 'text8.zip'\n",
        "\n",
        "if not os.path.exists(filename):\n",
        "  print(\"Downloading text8...\")\n",
        "  urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "# Extract the dataset\n",
        "with zipfile.ZipFile(filename) as f:\n",
        "  text = f.read(f.namelist()[0]).decode('utf-8')\n",
        "\n",
        "print(f\"First 300 characters:\\n{text[:300]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUxvUF1f3iWZ",
        "outputId": "a30b7290-425f-481a-d688-c0c3c1d7fc22"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 17005207\n",
            "Total filtered tokens: 10890638\n",
            "Total subsampled tokens: 4130359\n",
            "Unique words: 253702\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import random\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def subsample_tokens(tokens, threshold=1e-5):\n",
        "  subsampled_tokens = []\n",
        "  word_counts = Counter(tokens)\n",
        "  total_counts  = sum(word_counts.values())\n",
        "\n",
        "  for token in tokens:\n",
        "    normalized_freq = word_counts[token]/total_counts\n",
        "    p_keep = (threshold/normalized_freq)**0.5\n",
        "\n",
        "    p_keep = min(1.0, max(0.0, p_keep)) # clamp [0, 1]\n",
        "\n",
        "    if random.random() < p_keep:\n",
        "      subsampled_tokens.append(token)\n",
        "\n",
        "  return subsampled_tokens\n",
        "\n",
        "# English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Building vocab\n",
        "tokens = text.split()\n",
        "print(f\"Total tokens: {len(tokens)}\")\n",
        "\n",
        "# Filter out stop words\n",
        "filtered_tokens = [token for token in tokens if token.lower().strip() not in stop_words]\n",
        "print(f\"Total filtered tokens: {len(filtered_tokens)}\")\n",
        "\n",
        "subsampled_tokens = subsample_tokens(filtered_tokens)\n",
        "print(f\"Total subsampled tokens: {len(subsampled_tokens)}\")\n",
        "\n",
        "word_freq =  Counter(subsampled_tokens)\n",
        "print(f\"Unique words: {len(word_freq)}\")\n",
        "\n",
        "vocab = {word:idx for idx, (word, _) in enumerate(word_freq.items())}\n",
        "\n",
        "inv_vocab = {idx:word for word, idx in vocab.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYP5xjnr4yfb",
        "outputId": "9ffe45fb-63df-47cc-88af-4d1b86239403"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique words: 10000\n",
            "Filtered tokens: 9170125\n",
            "Filtered word freq: 10000\n"
          ]
        }
      ],
      "source": [
        "# Filter rare words\n",
        "vocab_size = 10000\n",
        "\n",
        "most_common = word_freq.most_common(vocab_size)#[:-1000-1:-1]\n",
        "vocab = {word:idx for idx, (word, _) in enumerate(most_common)}\n",
        "inv_vocab = {idx:word for word, idx in vocab.items()}\n",
        "print(f\"Unique words: {len(vocab)}\")\n",
        "\n",
        "# filter tokens to keep only those in vocab\n",
        "tokens = [token for token in tokens if token in vocab]\n",
        "print(f\"Filtered tokens: {len(tokens)}\")\n",
        "\n",
        "# filtered word freq\n",
        "filtered_word_freq = {word:freq for word, freq in most_common}\n",
        "print(f\"Filtered word freq: {len(filtered_word_freq)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUA4p3F3GSxs"
      },
      "outputs": [],
      "source": [
        "def generate_skipgram_pairs(tokens:list, window_size=8, max_len=1_000_000):\n",
        "  pairs = []\n",
        "\n",
        "  for i, center_word in enumerate(tokens):\n",
        "    if center_word not in vocab:\n",
        "      continue\n",
        "\n",
        "    start_idx = max(0, i - window_size)\n",
        "    end_idx = min(len(tokens), i + window_size + 1)\n",
        "\n",
        "    for j in range(start_idx, end_idx):\n",
        "\n",
        "      if i != j:\n",
        "        context_word = tokens[j]\n",
        "        if context_word in vocab:\n",
        "          pairs.append((vocab[center_word], vocab[context_word]))\n",
        "          if len(pairs) >= max_len:\n",
        "            return pairs\n",
        "\n",
        "  return pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_I4ixsIrT7k"
      },
      "outputs": [],
      "source": [
        "import heapq\n",
        "from collections import defaultdict\n",
        "\n",
        "# note that words are only leaf nodes\n",
        "class HuffmanNode:\n",
        "  def __init__(self, freq, word=None, left=None, right=None, idx=None):\n",
        "    self.freq = freq\n",
        "    self.word = word # None for internal nodes\n",
        "    self.left = left\n",
        "    self.right = right\n",
        "    self.idx = idx # unique index for internal nodes\n",
        "\n",
        "  def __lt__(self, other):\n",
        "    return self.freq < other.freq\n",
        "\n",
        "def build_huffman_tree(word_freq):\n",
        "  heap = []\n",
        "  word_to_leaf = {}\n",
        "\n",
        "  # initialize heap with leaf nodes\n",
        "  for word, freq in word_freq.items():\n",
        "    node = HuffmanNode(freq, word)\n",
        "    heapq.heappush(heap, node)\n",
        "    word_to_leaf[word] = node\n",
        "\n",
        "  next_internal_idx = len(word_freq) # internal node indices start after word indices\n",
        "\n",
        "  # build tree\n",
        "\n",
        "  while len(heap) > 1:\n",
        "    node1 = heapq.heappop(heap)\n",
        "    node2 = heapq.heappop(heap)\n",
        "\n",
        "    parent = HuffmanNode(\n",
        "        freq=node1.freq + node2.freq,\n",
        "        left=node1,\n",
        "        right=node2,\n",
        "        idx=next_internal_idx\n",
        "    )\n",
        "    next_internal_idx += 1\n",
        "    heapq.heappush(heap, parent)\n",
        "\n",
        "  root = heap[0]\n",
        "  return root, word_to_leaf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yn26ZGZRvsn5"
      },
      "outputs": [],
      "source": [
        "def extract_codes_and_paths(root, vocab):\n",
        "  word_idx_to_code = {}\n",
        "  word_idx_to_path = {}\n",
        "  max_path_len = 0\n",
        "\n",
        "  def dfs(node, code, path):\n",
        "    nonlocal max_path_len\n",
        "    if node.word is not None:\n",
        "      idx = vocab[node.word]\n",
        "      word_idx_to_code[idx] = code.copy()\n",
        "      word_idx_to_path[idx] = path.copy()\n",
        "      max_path_len = max(max_path_len, len(path))\n",
        "      return\n",
        "\n",
        "    # Go left (0)\n",
        "    dfs(node.left, code + [0], path + [node.idx])\n",
        "\n",
        "    # Go right (1)\n",
        "    dfs(node.right, code + [1], path + [node.idx])\n",
        "\n",
        "  dfs(root, [], [])\n",
        "  return word_idx_to_code, word_idx_to_path, max_path_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acHmDjJfzeL2"
      },
      "outputs": [],
      "source": [
        "def create_huffman_codes(word_freq, vocab):\n",
        "  root, _ = build_huffman_tree(word_freq)\n",
        "  codes, paths, max_path_len = extract_codes_and_paths(root, vocab)\n",
        "  return codes, paths, max_path_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1ZhJdfSP1Mg"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "def hierarchical_softmax_batched(hidden, target_word_idxs, codes, paths, node_embeddings, path_mask, vocab_size):\n",
        "  \"\"\"\n",
        "  hidden: (B, H)  - batch of context vectors\n",
        "  target_word_idxs: (B,) - not directly used here unless for lookup\n",
        "  codes: (B, L) - 0 or 1 (padded)\n",
        "  paths: (B, L) - internal node indices (original indices, padded with 0)\n",
        "  node_weights: (num_nodes, H) - Tensor of node weights, ordered according to sorted original indices + 0 for padding\n",
        "  path_mask: (B, L) - 1 for valid path steps, 0 for padding\n",
        "  vocab_size: int - size of the vocabulary\n",
        "  \"\"\"\n",
        "  B, L = paths.shape\n",
        "  H = hidden.size(1)\n",
        "\n",
        "  # Remap original paths indices to tensor indices\n",
        "  # Original internal node indices start from vocab_size\n",
        "  # Tensor indices start from 0 (for padding) and then 1 onwards for remapped internal nodes\n",
        "  # Mapping: original_idx -> original_idx - vocab_size + 1 (if original_idx >= vocab_size), 0 if original_idx is padding (0)\n",
        "  mapped_paths = torch.zeros_like(paths, dtype=torch.long)\n",
        "  # Handle padding index (-1)\n",
        "  mapped_paths[paths == 0] = 0\n",
        "  # Handle internal node indices\n",
        "  internal_node_mask = paths >= vocab_size\n",
        "  mapped_paths[internal_node_mask] = paths[internal_node_mask] - vocab_size\n",
        "\n",
        "\n",
        "  # Get internal node vectors for each sample in the batch\n",
        "  # mapped_paths: (B, L) → internal_node_vecs: (B, L, H)\n",
        "  internal_node_vecs = node_embeddings(mapped_paths)  # (B, L, H)\n",
        "\n",
        "\n",
        "  # Expand hidden from (B, H) → (B, L, H) to align for dot product\n",
        "  hidden_expanded = hidden.unsqueeze(1).expand(-1, L, -1)  # (B, L, H)\n",
        "\n",
        "  # Dot product: (B, L)\n",
        "  dot_scores = torch.sum(hidden_expanded * internal_node_vecs, dim=2)\n",
        "\n",
        "  # Sigmoid\n",
        "  probs = torch.sigmoid(dot_scores)  # (B, L)\n",
        "\n",
        "  # Log probs\n",
        "  codes = codes.float()\n",
        "  losses = F.binary_cross_entropy(probs, codes, reduction='none')\n",
        "\n",
        "  # Apply mask\n",
        "  losses = losses * path_mask.float() # (B, L)\n",
        "\n",
        "  # Normalize per sequence length\n",
        "  total_losses = losses.sum(dim=1)/path_mask.sum(dim=1).clamp(min=1e-8)\n",
        "\n",
        "  return total_losses.mean()  # scalar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgU-SEYjP6hP"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def pad_codes_and_paths(pairs, max_path_len, huffman_codes, huffman_paths, pad_idx=0, batch_size=32):\n",
        "\n",
        "  \"\"\"\n",
        "  huffman_codes: dict[word_idx] = list of 0/1\n",
        "  huffman_paths: dict[word_idx] = list of node indices\n",
        "  \"\"\"\n",
        "\n",
        "  padded_paths = []\n",
        "  padded_codes = []\n",
        "  path_masks = []\n",
        "\n",
        "  pairs_len = len(pairs)\n",
        "  for idx, (_, t) in enumerate(pairs):\n",
        "    path = huffman_paths[t]\n",
        "    code = huffman_codes[t]\n",
        "\n",
        "    # Padding\n",
        "    pad_len = max_path_len - len(path)\n",
        "    padded_path = path + [pad_idx] * pad_len\n",
        "    padded_code = code + [0] * pad_len\n",
        "    mask = [1] * len(path) + [0] * pad_len\n",
        "\n",
        "    padded_paths.append(padded_path)\n",
        "    padded_codes.append(padded_code)\n",
        "    path_masks.append(mask)\n",
        "\n",
        "    if (idx + 1) % batch_size == 0 or idx == pairs_len-1:\n",
        "      start_index = (math.ceil((idx + 1)/batch_size) - 1) * batch_size\n",
        "      batch = pairs[start_index: idx+1]\n",
        "      center_word_indices, context_word_indices = zip(*batch)\n",
        "\n",
        "      yield (torch.tensor(center_word_indices),\n",
        "             torch.tensor(context_word_indices),\n",
        "             torch.tensor(padded_codes, dtype=torch.long),\n",
        "             torch.tensor(padded_paths, dtype=torch.long),\n",
        "             torch.tensor(path_masks, dtype=torch.float)\n",
        "             )\n",
        "\n",
        "      padded_paths.clear()\n",
        "      padded_codes.clear()\n",
        "      path_masks.clear()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGsyNMovLog8"
      },
      "outputs": [],
      "source": [
        "class Word2VecHS(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, paths):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    # Create a list of unique internal node indices, including 0 for padding\n",
        "    internal_node_indices = sorted(list(set(idx for path in paths.values() for idx in path) | {0})) # Include 0 for padding. note that node indices starts at the end of vocab size\n",
        "\n",
        "\n",
        "    self.node_embeddings = nn.Embedding(len(internal_node_indices), embedding_dim, padding_idx=0) # create embedding for each node\n",
        "    self.dropout = nn.Dropout(0.6)\n",
        "\n",
        "  def forward(self, codes, paths, center_words_indices, context_words_indices, path_mask):\n",
        "    batch_size = center_words_indices.shape[0]\n",
        "    embeddings = self.embedding(center_words_indices)\n",
        "    embeddings = self.dropout(embeddings)\n",
        "\n",
        "\n",
        "    batch_loss = hierarchical_softmax_batched(\n",
        "        hidden=embeddings,\n",
        "        target_word_idxs=context_words_indices,\n",
        "        codes=codes,\n",
        "        paths=paths,\n",
        "        node_embeddings=self.node_embeddings, # Pass the tensor\n",
        "        path_mask=path_mask,\n",
        "        vocab_size=self.vocab_size\n",
        "      )\n",
        "\n",
        "    return batch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgYlOYVTO_dk",
        "outputId": "63607d4a-50fb-4500-dcc1-d51fd7781ab2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Word2VecHS(\n",
              "  (embedding): Embedding(10000, 300)\n",
              "  (node_embeddings): Embedding(10000, 300, padding_idx=0)\n",
              "  (dropout): Dropout(p=0.6, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "codes, paths, max_path_len = create_huffman_codes(filtered_word_freq, vocab=vocab)\n",
        "model = Word2VecHS(len(vocab), 300, paths)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQfkLMvfqwB7"
      },
      "outputs": [],
      "source": [
        "pairs = generate_skipgram_pairs(tokens, max_len=1_000_000)\n",
        "#batches = generate_skipgram_batches(pairs, batch_size=1)\n",
        "padded_batches = pad_codes_and_paths(pairs, max_path_len, codes, paths, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kdSuf22U0bK",
        "outputId": "c4749cd5-0de6-4dcf-806c-280e193cf010"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1000000"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-HBUCdJPreT"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF9MyOTYCpyz"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=1, factor=0.5, threshold=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKAn-Nv3P1k6",
        "outputId": "e93be76b-41f7-4bbe-e0c9-67bdf060f8a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40, Loss: 11.909344181529045, LR: 0.001\n",
            "Epoch 2/40, Loss: 5.808248297450065, LR: 0.001\n",
            "Epoch 3/40, Loss: 3.807633185948372, LR: 0.001\n",
            "Epoch 4/40, Loss: 2.772666641082764, LR: 0.001\n",
            "Epoch 5/40, Loss: 2.152166331498146, LR: 0.001\n",
            "Epoch 6/40, Loss: 1.7515308139390946, LR: 0.001\n",
            "Epoch 7/40, Loss: 1.4858631509661675, LR: 0.001\n",
            "Epoch 8/40, Loss: 1.2952395974769593, LR: 0.001\n",
            "Epoch 9/40, Loss: 1.1529252299599648, LR: 0.001\n",
            "Epoch 10/40, Loss: 1.0489013797821998, LR: 0.001\n",
            "Epoch 11/40, Loss: 0.9673258307271003, LR: 0.001\n",
            "Epoch 12/40, Loss: 0.9046352724394798, LR: 0.001\n",
            "Epoch 13/40, Loss: 0.8542623966655731, LR: 0.001\n",
            "Epoch 14/40, Loss: 0.8157607867102623, LR: 0.001\n",
            "Epoch 15/40, Loss: 0.7810280789175034, LR: 0.001\n",
            "Epoch 16/40, Loss: 0.7547019110040665, LR: 0.001\n",
            "Epoch 17/40, Loss: 0.7325589862709045, LR: 0.001\n",
            "Epoch 18/40, Loss: 0.714932128390789, LR: 0.001\n",
            "Epoch 19/40, Loss: 0.6979182883553505, LR: 0.001\n",
            "Epoch 20/40, Loss: 0.6850562409195899, LR: 0.001\n",
            "Epoch 21/40, Loss: 0.6737167777647972, LR: 0.001\n",
            "Epoch 22/40, Loss: 0.6639752472081184, LR: 0.001\n",
            "Epoch 23/40, Loss: 0.6559887775287628, LR: 0.001\n",
            "Epoch 24/40, Loss: 0.6485856815314293, LR: 0.001\n",
            "Epoch 25/40, Loss: 0.6424151053433418, LR: 0.001\n",
            "Epoch 26/40, Loss: 0.6363362081794739, LR: 0.001\n",
            "Epoch 27/40, Loss: 0.6318357237992287, LR: 0.001\n",
            "Epoch 28/40, Loss: 0.6277442938332558, LR: 0.001\n",
            "Epoch 29/40, Loss: 0.6233037744064331, LR: 0.001\n",
            "Epoch 30/40, Loss: 0.6199342754230499, LR: 0.001\n",
            "Epoch 31/40, Loss: 0.6169203655724526, LR: 0.001\n",
            "Epoch 32/40, Loss: 0.6140976357069016, LR: 0.001\n",
            "Epoch 33/40, Loss: 0.610479837908268, LR: 0.001\n",
            "Epoch 34/40, Loss: 0.608790130194664, LR: 0.001\n",
            "Epoch 35/40, Loss: 0.6066960755081177, LR: 0.001\n",
            "Epoch 36/40, Loss: 0.6038744073171616, LR: 0.001\n",
            "Epoch 37/40, Loss: 0.6024134522185326, LR: 0.001\n",
            "Epoch 38/40, Loss: 0.6003890723233223, LR: 0.001\n",
            "Epoch 39/40, Loss: 0.5981555532755852, LR: 0.001\n",
            "Epoch 40/40, Loss: 0.5970449008154869, LR: 0.001\n"
          ]
        }
      ],
      "source": [
        "epochs = 40\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "  running_loss = 0.0\n",
        "  batch_count = 0\n",
        "\n",
        "  # Regenerate batches each epoch\n",
        "  padded_batches = pad_codes_and_paths(pairs, max_path_len, codes, paths, batch_size=32)\n",
        "\n",
        "\n",
        "  for center_batch, context_batch, codes_batch, paths_batch, path_mask_batch in padded_batches:\n",
        "    center_batch = center_batch.to(device)\n",
        "    context_batch = context_batch.to(device)\n",
        "    codes_batch = codes_batch.to(device)\n",
        "    paths_batch = paths_batch.to(device)\n",
        "    path_mask_batch = path_mask_batch.to(device)\n",
        "\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = model(codes_batch, paths_batch, center_batch, context_batch, path_mask_batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    batch_count += 1\n",
        "  scheduler.step(running_loss/batch_count)\n",
        "\n",
        "  print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/batch_count}, LR: {scheduler.get_last_lr()[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iai9aE-ZSVcJ"
      },
      "outputs": [],
      "source": [
        "# use model to predict next word\n",
        "def predict_next_topk_words(word_idx, model=model, topk=5):\n",
        "  topk_words = []\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    last_embedding = model.embedding(torch.tensor([word_idx]).to(device))  # shape: (1, D)\n",
        "\n",
        "    # Normalize embeddings to unit vectors\n",
        "    normalized_embeddings = F.normalize(model.embedding.weight, dim=1)\n",
        "    normalized_last = F.normalize(last_embedding, dim=1)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    cos_similarities = torch.matmul(normalized_last, normalized_embeddings.T).squeeze(0)\n",
        "\n",
        "    topk = torch.topk(cos_similarities, k=topk)\n",
        "    for i in topk.indices:\n",
        "      topk_words.append(inv_vocab[i.item()])  # Convert index back to word\n",
        "\n",
        "    return topk_words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYaMERT3-BFs",
        "outputId": "5e8a5fbc-f014-4a14-9f5b-c4c9bb60042a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['queen', 'elizabeth', 'governor', 'represented', 'parliament']\n"
          ]
        }
      ],
      "source": [
        "last_word = \"queen\"\n",
        "last_word_idx = vocab[last_word]\n",
        "\n",
        "predictions = predict_next_topk_words(last_word_idx)\n",
        "print(predictions)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
