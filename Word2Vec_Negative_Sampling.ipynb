{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV7N0FEh6S_c",
        "outputId": "f5dd08a5-7c92-4eac-85f3-16f3ca00852d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading text8...\n",
            "First 300 characters:\n",
            " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organiz\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import urllib.request\n",
        "\n",
        "# Download the dataset\n",
        "url = 'http://mattmahoney.net/dc/text8.zip'\n",
        "filename = 'text8.zip'\n",
        "\n",
        "if not os.path.exists(filename):\n",
        "  print(\"Downloading text8...\")\n",
        "  urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "# Extract the dataset\n",
        "with zipfile.ZipFile(filename) as f:\n",
        "  text = f.read(f.namelist()[0]).decode('utf-8')\n",
        "\n",
        "print(f\"First 300 characters:\\n{text[:300]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import random\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def subsample_tokens(tokens, threshold=1e-5):\n",
        "  subsampled_tokens = []\n",
        "  word_counts = Counter(tokens)\n",
        "  total_counts  = sum(word_counts.values())\n",
        "\n",
        "  for token in tokens:\n",
        "    normalized_freq = word_counts[token]/total_counts\n",
        "    p_discard = 1 - (threshold/normalized_freq)**0.5\n",
        "\n",
        "    if random.random() > p_discard:\n",
        "      subsampled_tokens.append(token)\n",
        "\n",
        "  return subsampled_tokens\n",
        "\n",
        "# English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Building vocab\n",
        "tokens = text.split()\n",
        "print(f\"Total tokens: {len(tokens)}\")\n",
        "\n",
        "# Filter out stop words\n",
        "filtered_tokens = [token for token in tokens if token.lower().strip() not in stop_words]\n",
        "print(f\"Total filtered tokens: {len(filtered_tokens)}\")\n",
        "\n",
        "subsampled_tokens = subsample_tokens(filtered_tokens)\n",
        "print(f\"Total subsampled tokens: {len(subsampled_tokens)}\")\n",
        "\n",
        "word_freq =  Counter(subsampled_tokens)\n",
        "print(f\"Unique words: {len(word_freq)}\")\n",
        "\n",
        "vocab = {word:idx for idx, (word, _) in enumerate(word_freq.items())}\n",
        "\n",
        "inv_vocab = {idx:word for word, idx in vocab.items()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptfxiHzk647Q",
        "outputId": "b74ad54e-a3b3-4dfd-be60-6aa0671fc362"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens: 17005207\n",
            "Total filtered tokens: 10890638\n",
            "Total subsampled tokens: 4132620\n",
            "Unique words: 253702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter rare words\n",
        "vocab_size = 10000\n",
        "\n",
        "most_common = word_freq.most_common(vocab_size)#[:-1000-1:-1]\n",
        "vocab = {word:idx for idx, (word, _) in enumerate(most_common)}\n",
        "inv_vocab = {idx:word for word, idx in vocab.items()}\n",
        "print(f\"Unique words: {len(vocab)}\")\n",
        "\n",
        "# filter tokens to keep only those in vocab\n",
        "tokens = [token for token in tokens if token in vocab]\n",
        "print(f\"Filtered tokens: {len(tokens)}\")\n",
        "\n",
        "# filtered word freq\n",
        "filtered_word_freq = {word:freq for word, freq in most_common}\n",
        "print(f\"Filtered word freq: {len(filtered_word_freq)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxv0SmL07Iid",
        "outputId": "21509074-900e-4fe1-f128-b1c2aa78038f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words: 10000\n",
            "Filtered tokens: 9170278\n",
            "Filtered word freq: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "\n",
        "def generate_data_indices(tokens: list, vocab: dict, word_freq: dict,\n",
        "                          k=5, C=5, batch_size=32, max_data_points=None,\n",
        "                          device='cpu'):\n",
        "  \"\"\"\n",
        "  Generate skip-gram with negative sampling data using PyTorch.\n",
        "\n",
        "  Args:\n",
        "      tokens (list): list of tokens (strings)\n",
        "      vocab (dict): mapping word -> index\n",
        "      word_freq (dict): mapping word -> frequency (for negative sampling)\n",
        "      k (int): number of negative samples\n",
        "      C (int): maximum window size\n",
        "      batch_size (int): batch size\n",
        "      max_data_points (int or None): maximum number of center-target pairs\n",
        "      device (str): 'cpu' or 'cuda'\n",
        "\n",
        "  Yields:\n",
        "      centers (Tensor): shape (B,)\n",
        "      targets (Tensor): shape (B,)\n",
        "      negatives (Tensor): shape (B, k)\n",
        "  \"\"\"\n",
        "\n",
        "  vocab_size = len(vocab)\n",
        "  token_len = len(tokens)\n",
        "  batch = []\n",
        "  count = 0\n",
        "\n",
        "  # --- Build negative sampling distribution (unigram^0.75) ---\n",
        "  freqs = torch.tensor([word_freq[w] for w in vocab.keys()], dtype=torch.float32)\n",
        "  freqs = freqs ** 0.75\n",
        "  neg_sampling_dist = freqs / freqs.sum()\n",
        "\n",
        "  for idx, center_word in enumerate(tokens):\n",
        "    if max_data_points is not None and count >= max_data_points:\n",
        "        break\n",
        "\n",
        "    center_idx = vocab[center_word]\n",
        "\n",
        "    # dynamic window size (1..C)\n",
        "    window_size = random.randint(1, C)\n",
        "\n",
        "    for j in range(-window_size, window_size + 1):\n",
        "      if j == 0 or not (0 <= idx + j < token_len):\n",
        "        continue\n",
        "\n",
        "      target_word = tokens[idx + j]\n",
        "\n",
        "      target_idx = vocab[target_word]\n",
        "\n",
        "      # sample negatives using PyTorch multinomial\n",
        "      neg_samples = torch.multinomial(neg_sampling_dist, num_samples=k*2, replacement=True)\n",
        "\n",
        "      # remove true target if accidentally sampled\n",
        "      neg_samples = neg_samples[neg_samples != target_idx]\n",
        "      neg_samples = neg_samples[:k]  # take exactly k negatives\n",
        "\n",
        "      batch.append((center_idx, target_idx, neg_samples))\n",
        "      count += 1\n",
        "\n",
        "      if len(batch) == batch_size:\n",
        "        # convert batch to tensors\n",
        "        centers = torch.tensor([c for c, _, _ in batch], dtype=torch.long, device=device)\n",
        "        targets = torch.tensor([t for _, t, _ in batch], dtype=torch.long, device=device)\n",
        "        negatives = torch.stack([n for _, _, n in batch]).to(device)  # shape (B, k)\n",
        "\n",
        "        yield centers, targets, negatives\n",
        "        batch = []\n",
        "\n",
        "  # leftover batch\n",
        "  if batch:\n",
        "      centers = torch.tensor([c for c, _, _ in batch], dtype=torch.long, device=device)\n",
        "      targets = torch.tensor([t for _, t, _ in batch], dtype=torch.long, device=device)\n",
        "      negatives = torch.stack([n for _, _, n in batch]).to(device)\n",
        "      yield centers, targets, negatives\n"
      ],
      "metadata": {
        "id": "qUXOuvV5UY1a"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "def negative_sampling_loss(center_embeds, true_target_embeds, context_embeds):\n",
        "  \"\"\"\n",
        "  center_embeds: (B, D) - embeddings of center words (from center embedding matrix)\n",
        "  true_target_embeds: (B, D) - embeddings of true context words (from context embedding matrix)\n",
        "  context_embeds: (B, K, D) - embedding of context or target words (from context embedding matrix)\n",
        "  K is the number of samples to be drawn for each center word\n",
        "  \"\"\"\n",
        "\n",
        "  pos_logits =  torch.sum(center_embeds * true_target_embeds, dim=1) # (B,)\n",
        "  pos_loss = F.logsigmoid(pos_logits) # (B,)\n",
        "\n",
        "  neg_logits = torch.bmm(context_embeds, center_embeds.unsqueeze(2)).squeeze(2) # (B, K)\n",
        "  neg_loss = F.logsigmoid(-neg_logits).sum(dim=1) # (B,)\n",
        "\n",
        "  loss = -(pos_loss + neg_loss).mean()\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "DULgm_-mbWEK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class Word2VecNS(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.center_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.context_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "  def forward(self, center_word_indices, true_target_indices, context_word_indices):\n",
        "    center_embeds = self.center_embedding(center_word_indices)\n",
        "    context_embeds = self.context_embedding(context_word_indices)\n",
        "    true_target_embeds = self.context_embedding(true_target_indices)\n",
        "    return center_embeds, true_target_embeds, context_embeds\n"
      ],
      "metadata": {
        "id": "f4r-2AiXaj6I"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2VecNS(vocab_size=len(vocab), embedding_dim=300)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBnYVyTgMo2h",
        "outputId": "90f5caf3-a01d-45de-83b1-f868f6ef66c0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Word2VecNS(\n",
              "  (center_embedding): Embedding(10000, 300)\n",
              "  (context_embedding): Embedding(10000, 300)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "SDwo0ceugMet"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "\n",
        "epochs = 30\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "  running_loss = 0.0\n",
        "  batch_count = 0\n",
        "\n",
        "  data = generate_data_indices(tokens, vocab=vocab, word_freq=filtered_word_freq, k=5, C=5, batch_size=32, max_data_points=1_000_000, device=device)\n",
        "\n",
        "  for center_idxs, target_idxs, context_idxs in data:\n",
        "\n",
        "    center_idxs = center_idxs.to(device)\n",
        "    target_idxs = target_idxs.to(device)\n",
        "    context_idxs = context_idxs.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    center_embeds, target_embeds, context_embeds = model(center_idxs, target_idxs, context_idxs)\n",
        "\n",
        "    loss = negative_sampling_loss(center_embeds, target_embeds, context_embeds)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    batch_count += 1\n",
        "\n",
        "\n",
        "  print(f\"Epoch {epoch+1} Loss: {running_loss/batch_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KBzDBncgQXS",
        "outputId": "6d797e43-8d0e-4a49-a63f-1a6ff02ce34a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 29.08660999885386\n",
            "Epoch 2 Loss: 14.112728043267763\n",
            "Epoch 3 Loss: 8.43846134388932\n",
            "Epoch 4 Loss: 5.909612085083861\n",
            "Epoch 5 Loss: 4.471298608461067\n",
            "Epoch 6 Loss: 3.592523534725638\n",
            "Epoch 7 Loss: 3.0125300137918036\n",
            "Epoch 8 Loss: 2.634447143848733\n",
            "Epoch 9 Loss: 2.389984536399262\n",
            "Epoch 10 Loss: 2.204498428423895\n",
            "Epoch 11 Loss: 2.083149959297092\n",
            "Epoch 12 Loss: 1.9953826344771528\n",
            "Epoch 13 Loss: 1.933586987579259\n",
            "Epoch 14 Loss: 1.8866075099800248\n",
            "Epoch 15 Loss: 1.8494391960373602\n",
            "Epoch 16 Loss: 1.827853235629969\n",
            "Epoch 17 Loss: 1.8115166856584173\n",
            "Epoch 18 Loss: 1.7966397875170552\n",
            "Epoch 19 Loss: 1.7863305828742684\n",
            "Epoch 20 Loss: 1.7804902502636633\n",
            "Epoch 21 Loss: 1.7809785443179702\n",
            "Epoch 22 Loss: 1.770274683031227\n",
            "Epoch 23 Loss: 1.7664639709183525\n",
            "Epoch 24 Loss: 1.7617347584996523\n",
            "Epoch 25 Loss: 1.7697679223510325\n",
            "Epoch 26 Loss: 1.7629490103600323\n",
            "Epoch 27 Loss: 1.7621526014777578\n",
            "Epoch 28 Loss: 1.7617466407990152\n",
            "Epoch 29 Loss: 1.762099759918562\n",
            "Epoch 30 Loss: 1.759415338001871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use model to predict next word\n",
        "def predict_next_topk_words(word_idx, model=model, topk=5):\n",
        "  topk_words = []\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    last_embedding = model.center_embedding(torch.tensor([word_idx]).to(device))  # shape: (1, D)\n",
        "\n",
        "    # Normalize embeddings to unit vectors\n",
        "    normalized_embeddings = F.normalize(model.center_embedding.weight, dim=1)\n",
        "    normalized_last = F.normalize(last_embedding, dim=1)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    cos_similarities = torch.matmul(normalized_last, normalized_embeddings.T).squeeze(0)\n",
        "\n",
        "    topk = torch.topk(cos_similarities, k=topk)\n",
        "    for i in topk.indices:\n",
        "      topk_words.append(inv_vocab[i.item()])  # Convert index back to word\n",
        "\n",
        "    return topk_words\n",
        "\n"
      ],
      "metadata": {
        "id": "D6gmSa9wnHV0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_word = \"football\"\n",
        "last_word_idx = vocab[last_word]\n",
        "\n",
        "predictions = predict_next_topk_words(last_word_idx, topk=10)\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YheFk9bboZ2W",
        "outputId": "f29c60bf-6d72-4c39-b9f7-9f542d5f6aca"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['football', 'league', 'game', 'nfl', 'american', 'rugby', 'played', 'sport', 'players', 'sports']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-962X29hiQXf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}