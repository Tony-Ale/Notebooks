{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“˜ <a href=\"https://download.pytorch.org/tutorial/data.zip\" target=\"_blank\">Download data</a>"
      ],
      "metadata": {
        "id": "wTfNQfCFSOhk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ksOtN0x8CMoF"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "# Path to your zip file\n",
        "zip_path = \"data.zip\"\n",
        "\n",
        "# Extract all contents\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"data\")  # Extract to 'data' folder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import unicodedata\n",
        "\n",
        "# I will use \"_\" to represent an out-of-vocabulary character, that is, any character we are not handling in our model\n",
        "allowed_chars = string.ascii_letters  + \" .,;'\" + \"_\"\n",
        "n_letters = len(allowed_chars)\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in allowed_chars\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "s_arteyDCou5"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (f\"converting 'ÅšlusÃ rski' to {unicodeToAscii('ÅšlusÃ rski')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka5h4PqzytNr",
        "outputId": "91763ed2-a632-4f8f-ad1c-5213d5762eba"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "converting 'ÅšlusÃ rski' to Slusarski\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "def letter_to_index(letter:str):\n",
        "  if letter not in allowed_chars:\n",
        "    return allowed_chars.find(\"_\")\n",
        "  else:\n",
        "    return allowed_chars.find(letter)\n",
        "\n",
        "def name_to_tensor(name:str):\n",
        "  tensor = torch.zeros(len(name), 1, n_letters)\n",
        "\n",
        "  for idx, letter in enumerate(name):\n",
        "    tensor[idx][0][letter_to_index(letter)] = 1\n",
        "  return tensor"
      ],
      "metadata": {
        "id": "ibPVUdezDvcf"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(name_to_tensor(\"Albert\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xW6i-NKFg2_",
        "outputId": "f8518cd8-074e-4e6d-c491-f5a007448680"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Creating a custom Dataset class for loading names from text files\n",
        "class NamesDataset(Dataset):\n",
        "  def __init__(self, path):\n",
        "    self.path = path # Path to the folder containing .txt files\n",
        "    labels_set = set() # Used to collect all unique labels\n",
        "    self.data = [] # This will store all the names\n",
        "    self.data_tensors = [] # This will store the tensor versions of the names\n",
        "    self.labels = [] # This will store the label (e.g., \"English\") for each name\n",
        "    self.label_tensors = [] # This will store the tensor versions of the labels\n",
        "\n",
        "    # Find all .txt files in the given path\n",
        "    text_files = glob.glob(os.path.join(path, '*.txt'))\n",
        "\n",
        "    for filename in text_files:\n",
        "      basename = os.path.basename(filename) # e.g., \"English.txt\"\n",
        "      label = os.path.splitext(basename)[0] # e.g., \"English\"\n",
        "      labels_set.add(label) # Add label to the set of all unique labels\n",
        "      with open(filename, encoding='utf-8') as file:\n",
        "        names = file.read().strip().split(\"\\n\") # Split file by new lines\n",
        "        for name in names:\n",
        "          self.data.append(name)\n",
        "          self.labels.append(label)\n",
        "\n",
        "          self.data_tensors.append(name_to_tensor(name))\n",
        "\n",
        "    # Create a list of all unique labels (e.g., ['English', 'French', ...])\n",
        "    self.label_uniq = list(labels_set)\n",
        "\n",
        "    # Convert each label to a tensor (as an index from label_uniq)\n",
        "    for label in self.labels:\n",
        "      self.label_tensors.append(torch.tensor([self.label_uniq.index(label)], dtype=torch.long))\n",
        "\n",
        "  # Return total number of data samples (i.e., number of names)\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  # Return a specific item: (name_tensor, label_tensor, name_as_string)\n",
        "  def __getitem__(self, idx):\n",
        "    data_item = self.data[idx]\n",
        "    return self.data_tensors[idx], self.label_tensors[idx], data_item"
      ],
      "metadata": {
        "id": "tPGBEQ5Ry0bC"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = NamesDataset(\"data/data/names\")"
      ],
      "metadata": {
        "id": "RnCwMrd83EpJ"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we cant use a dataloader to batch the data because each name has different lengths\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(all_data, [0.8, 0.2])"
      ],
      "metadata": {
        "id": "KbrC6SUP3Ilj"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Name: {train_dataset[0][-1]}, Tensor Shape: {train_dataset[0][0].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmRfGx179GO6",
        "outputId": "224a8167-a298-4fbd-fc31-e5ce26267ca2"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: Toal, Tensor Shape: torch.Size([4, 1, 58])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the network\n",
        "import torch.nn as nn\n",
        "class RNNcell(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super().__init__()\n",
        "\n",
        "    # This layer will handle the input at the current time step (x_t)\n",
        "    self.x_t = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "    # This layer will handle the hidden state from the previous time step (h_t)\n",
        "    self.h_t = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "  def forward(self, x, h_x):\n",
        "    # x is of shape (batch_size, n_rows, n_cols)\n",
        "    return torch.tanh(self.x_t(x) + self.h_t(h_x))\n",
        "\n"
      ],
      "metadata": {
        "id": "dUIjDEju9EyT"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, RNNcell=RNNcell):\n",
        "    super().__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.RNNcell = RNNcell(input_size, hidden_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    output = []\n",
        "    # x is of shape (batch_size, n_rows, n_cols)\n",
        "    n_steps = x.shape[0] # this is the number of data points\n",
        "\n",
        "    # Initialize the hidden state as a tensor of zeros; hidden state for each data point\n",
        "    h_x = torch.zeros(x.shape[1], self.hidden_size) # hidden state\n",
        "\n",
        "    for i in range(n_steps):\n",
        "      # At each step, feed the character and previous hidden state into the RNN cell\n",
        "      # x[i] has shape (1, 58)\n",
        "\n",
        "      h_x = self.RNNcell(x[i], h_x)\n",
        "      output.append(h_x)\n",
        "\n",
        "    return torch.stack(output), h_x"
      ],
      "metadata": {
        "id": "n8_b9ZoqXOUN"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class charRNN(nn.Module):\n",
        "  def __init__(self, input_size=n_letters, hidden_size=128, output_size=len(all_data.label_uniq) ):\n",
        "    super().__init__()\n",
        "    self.rnn = RNN(input_size, hidden_size)\n",
        "    self.h2o = nn.Linear(hidden_size, output_size) # hidden to output\n",
        "\n",
        "  def forward(self, x):\n",
        "    rnn_output, hidden = self.rnn(x)\n",
        "    output = self.h2o(hidden)\n",
        "    return output"
      ],
      "metadata": {
        "id": "mcUGozwruiMC"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = charRNN()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8S46D_0C04PF",
        "outputId": "cebb0fe7-9a7e-490c-c4aa-3c89c5795ee1"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "charRNN(\n",
              "  (rnn): RNN(\n",
              "    (RNNcell): RNNcell(\n",
              "      (x_t): Linear(in_features=58, out_features=128, bias=True)\n",
              "      (h_t): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (h2o): Linear(in_features=128, out_features=18, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "NKnVx4JbzpTi"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "# Train the model\n",
        "epochs = 27\n",
        "batch_size = 64\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "  running_loss = 0.0\n",
        "  batches = list(range(len(train_dataset)))\n",
        "  random.shuffle(batches)\n",
        "  batches = np.array_split(batches, len(train_dataset)//batch_size)\n",
        "  for batch in batches:\n",
        "    batch_loss = 0.0\n",
        "    for i in batch:\n",
        "      data, target, _ = train_dataset[i]\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = model(data)\n",
        "      loss = criterion(output, target)\n",
        "      batch_loss += loss\n",
        "    batch_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    running_loss += batch_loss.item()\n",
        "  print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_dataset)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "JOx8jlgk0xGV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6fb254a-1d67-444e-d11f-de0f59599c03"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.5168744960134086\n",
            "Epoch 2, Loss: 1.07276706766815\n",
            "Epoch 3, Loss: 0.9443094840233828\n",
            "Epoch 4, Loss: 0.868816487726803\n",
            "Epoch 5, Loss: 0.8022869074478245\n",
            "Epoch 6, Loss: 0.7566109913817676\n",
            "Epoch 7, Loss: 0.7251738077973666\n",
            "Epoch 8, Loss: 0.6961696346848275\n",
            "Epoch 9, Loss: 0.668678914565375\n",
            "Epoch 10, Loss: 0.6420803967923631\n",
            "Epoch 11, Loss: 0.6185171896909569\n",
            "Epoch 12, Loss: 0.5968777989094165\n",
            "Epoch 13, Loss: 0.5741230902903404\n",
            "Epoch 14, Loss: 0.5545712854615777\n",
            "Epoch 15, Loss: 0.5383377420798334\n",
            "Epoch 16, Loss: 0.5194439397505479\n",
            "Epoch 17, Loss: 0.4994999759075502\n",
            "Epoch 18, Loss: 0.48689501603246477\n",
            "Epoch 19, Loss: 0.4668450525362198\n",
            "Epoch 20, Loss: 0.45308816020248244\n",
            "Epoch 21, Loss: 0.4383830129877568\n",
            "Epoch 22, Loss: 0.4217781994440784\n",
            "Epoch 23, Loss: 0.4076407665333445\n",
            "Epoch 24, Loss: 0.39048308168223606\n",
            "Epoch 25, Loss: 0.3748656043079989\n",
            "Epoch 26, Loss: 0.36341478405972644\n",
            "Epoch 27, Loss: 0.3547410526133118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topk_label(output, output_labels):\n",
        "  values, indices = output.topk(1)\n",
        "  label_idx = indices[0].item()\n",
        "  return output_labels[label_idx], label_idx"
      ],
      "metadata": {
        "id": "YCLp5Y713KqF"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine accuracy\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  correct = 0\n",
        "  total = len(test_dataset)\n",
        "  for data, target, _ in test_dataset:\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    output = model(data)\n",
        "    label, label_idx = get_topk_label(output, all_data.label_uniq)\n",
        "    if label_idx == target.item():\n",
        "      correct += 1\n",
        "  print(f\"Accuracy: {100*correct/total}\")"
      ],
      "metadata": {
        "id": "uOeMt005X2hG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35b50651-c917-46ee-911b-5bd0a9c833ec"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 81.58943697060289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test the model\n",
        "import random\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  indices = random.sample(range(len(test_dataset)), 10)\n",
        "  test_sample = [test_dataset[i] for i in indices]\n",
        "  for (data, target, name), idx in zip(test_sample, indices):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    output = model(data)\n",
        "    label, label_idx = get_topk_label(output, all_data.label_uniq)\n",
        "    print(f\"Name: {name}, Predicted: {label}, Actual: {all_data.label_uniq[target.item()]}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W5Ngw8cONMg4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "493da964-a05d-4adc-ec9e-723ef4b04a26"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: Alman, Predicted: English, Actual: Russian\n",
            "Name: Pointer, Predicted: English, Actual: English\n",
            "Name: Junin, Predicted: Russian, Actual: Russian\n",
            "Name: Maryanov, Predicted: Russian, Actual: Russian\n",
            "Name: Tsegoev, Predicted: Russian, Actual: Russian\n",
            "Name: Judin, Predicted: Russian, Actual: Russian\n",
            "Name: Kennard, Predicted: English, Actual: English\n",
            "Name: Ventura, Predicted: Spanish, Actual: English\n",
            "Name: Abadi, Predicted: Arabic, Actual: Arabic\n",
            "Name: Schlusser, Predicted: German, Actual: German\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to markdown --output=RNN RNN.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcL7wdoXLNj_",
        "outputId": "b918281f-ffe1-4161-ec5b-7ce48a27251c"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook RNN.ipynb to markdown\n",
            "[NbConvertApp] Writing 10853 bytes to RNN.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r_TsTDeJLQ0Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}